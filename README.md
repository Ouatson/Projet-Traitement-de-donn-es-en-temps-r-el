# ğŸš€ Pipeline Data Streaming : Ingestion Clients (Kafka & Spark)

Ce projet implÃ©mente un pipeline de donnÃ©es **Temps RÃ©el** (Real-Time) robuste sur un environnement Big Data (HDP Sandbox). Il simule l'arrivÃ©e continue de nouveaux clients, les ingÃ¨re via Kafka, les traite avec Spark Structured Streaming, et les stocke sur HDFS selon leur validitÃ©.

## ğŸ“‹ Architecture du Pipeline

Le flux de donnÃ©es traverse les composants suivants :

1.  **Source** : Fichier CSV (`customers.csv`) simulant une base de donnÃ©es source.
2.  **Ingestion (Producer)** : Script Python (`kafka_producer.py`) qui publie les enregistrements en JSON dans **Kafka**.
3.  **Traitement (Spark Engine)** : Job Spark Streaming (`spark_streaming_job.py`) qui :
    * Lit le flux Kafka en continu (`readStream`).
    * Parse la structure JSON et nettoie les types de donnÃ©es.
    * Filtre les donnÃ©es : sÃ©pare les clients "USA" valides des donnÃ©es incomplÃ¨tes ("Alerts").
4.  **Stockage (HDFS)** :
    * `/user/maria_dev/customers_usa` : DonnÃ©es propres (Parquet/JSON).
    * `/user/maria_dev/customers_alerts` : Rejets et erreurs pour analyse.

---

## ğŸ“‚ Structure du Projet

Voici l'organisation recommandÃ©e des fichiers pour ce projet :

```text
customer-streaming-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ customers.csv             # Fichier source (Dataset)
â”‚
â”œâ”€â”€ jars/                         # DÃ©pendances Java pour Spark
â”‚   â”œâ”€â”€ kafka-clients-1.1.1.jar
â”‚   â””â”€â”€ spark-sql-kafka-0-10_2.11-2.3.2.jar
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kafka_producer.py         # Le Producer Kafka (Python)
â”‚   â””â”€â”€ spark_streaming_job.py    # Le Job Spark Streaming (Pyspark)
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ reset_environment.sh      # Script de nettoyage (HDFS + Checkpoints)
â”‚
â”œâ”€â”€ requirements.txt              # DÃ©pendances (ex: kafka-python)
â””â”€â”€ README.md                     # Documentation du projet
```

---

## ğŸ› ï¸ PrÃ©-requis

* **Environnement** : Hortonworks Data Platform (HDP Sandbox) ou Cluster Spark/Kafka.
* **Spark** : Version 2.3+ (Compatible Structured Streaming).
* **Kafka** : Topic configurÃ©.
* **Python** : 2.7 (dans notre sandbox).

---

## ğŸš€ Installation et DÃ©marrage

### 1. Configuration de Kafka
CrÃ©ation du topic qui recevra les donnÃ©es brutes :

```bash
/usr/hdp/current/kafka-broker/bin/kafka-topics.sh \\
  --create \\
  --zookeeper sandbox-hdp.hortonworks.com:2181 \\
  --replication-factor 1 \\
  --partitions 1 \\
  --topic customers-raw
```

Et des deux autres topic (customers-raw ainsi que customers-alerts) de la mÃªme maniÃ¨re.

### 2. DÃ©marrage du Producer
Ce script va lire le fichier CSV depuis S3 avec boto3 et envoyer les messages un par un dans Kafka (le topic customers-raw).

```bash
python kafka_producer.py
```
*Laissez ce terminal ouvert ou lancez-le en arriÃ¨re-plan.*

### 3. ExÃ©cution du Job Spark
Soumettez le job Ã  YARN ou en local via `spark-submit`. Notez l'utilisation des `.jars` pour le connecteur Kafka.

```bash
spark-submit \\
  --jars spark-sql-kafka-0-10_2.11-2.3.2.jar,kafka-clients-1.1.1.jar \\
  python_spark_job.py
```

---

## âš™ï¸ Configuration Technique & Robustesse

Ce projet a Ã©tÃ© configurÃ© pour gÃ©rer les erreurs courantes :

### 1. Gestion des Pertes de DonnÃ©es (Data Loss)
Kafka peut supprimer des anciens messages (rÃ©tention) avant que Spark ne les lise. Pour Ã©viter que le job ne crash avec une erreur `OffsetOutOfRangeException`, nous utilisons :
```python
.option("failOnDataLoss", "false")
```

### 2. Checkpointing (TolÃ©rance aux pannes)
Spark utilise des dossiers de checkpoints locaux pour sauvegarder l'Ã©tat du flux (offsets).
* **Chemin** : `/tmp/checkpoint_customers_...`
Cela garantit la sÃ©mantique **"Exactly-Once"** (aucun doublon, aucune perte) en cas de redÃ©marrage.

---

## ğŸ§¹ ProcÃ©dure de Reset (DÃ©pannage)

Si vous rencontrez des erreurs de type `Metadata Log` ou `IllegalStateException` (conflit entre le checkpoint et HDFS), ou si vous souhaitez relancer le traitement depuis le dÃ©but (offset 0), **suivez impÃ©rativement cette procÃ©dure de nettoyage** :

**1. ArrÃªter le Producer et le Job Spark (Ctrl+C).**

**2. Supprimer les mÃ©tadonnÃ©es locales (Le Cerveau) :**
```bash
rm -rf /tmp/checkpoint_customers_usa
rm -rf /tmp/checkpoint_customers_alerts
```

**3. Supprimer les donnÃ©es sur HDFS (La Destination) :**
```bash
hdfs dfs -rm -r /user/maria_dev/customers_usa
hdfs dfs -rm -r /user/maria_dev/customers_alerts
```

**4. Relancer le Producer puis le Job Spark.**

---

## ğŸ“Š VÃ©rification des RÃ©sultats

Pour vÃ©rifier que les donnÃ©es arrivent bien sur HDFS :

```bash
# Lister les fichiers
hdfs dfs -ls /user/maria_dev/customers_usa

# Lire le contenu d'un fichier (exemple)
hdfs dfs -cat /user/maria_dev/customers_usa/part-00000-....json
```
