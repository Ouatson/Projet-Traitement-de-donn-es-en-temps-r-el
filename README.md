# ğŸš€ Pipeline Data Streaming : Ingestion Clients (Kafka & Spark)

Ce projet implÃ©mente un pipeline de donnÃ©es **Temps RÃ©el** (Real-Time) robuste sur un environnement Big Data (HDP Sandbox). Il simule l'arrivÃ©e continue de nouveaux clients, les ingÃ¨re via Kafka, les traite avec Spark Structured Streaming, et les stocke sur HDFS selon leur validitÃ©. Avec une partie de liaison Ã  une base de donnÃ©es en ligne, SupaBase, dont le lien est le suivant : https://supabase.com/dashboard/org/tvugxmxqzkwjuphokzai.

## ğŸ“‹ Architecture du Pipeline

Le flux de donnÃ©es traverse les composants suivants :

1.  **Source** : Fichier CSV (`customers.csv`) simulant une base de donnÃ©es source.
2.  **Ingestion (Producer)** : Script Python (`kafka_producer.py`) qui publie les enregistrements en JSON dans **Kafka**.
3.  **Traitement (Spark Engine)** : Job Spark Streaming (`spark_streaming_job_kafka_hdfs.py`) qui :
    * Lit le flux Kafka en continu (`readStream`).
    * Parse la structure JSON et nettoie les types de donnÃ©es.
    * Filtre les donnÃ©es : sÃ©pare les clients "USA" valides des donnÃ©es incomplÃ¨tes ("Alerts").
4.  **Stockage (HDFS)** :
    * `/user/maria_dev/customers_usa` : DonnÃ©es propres (Parquet/JSON).
    * `/user/maria_dev/customers_alerts` : Rejets et erreurs pour analyse.
5.  **Stockage BD (SupaBase)** : Job Spark Streaming (`spark_streaming_job_db.py`) qui envoie les messages kafka dans une base de donnÃ©e **SupaBase**.

---

## ğŸ“‚ Structure du Projet

Voici l'organisation recommandÃ©e des fichiers pour ce projet :

```text
customer-streaming-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ customers.csv                      # Fichier source (Dataset)
â”‚
â”œâ”€â”€ jars/                                  # DÃ©pendances Java pour Spark
â”‚   â”œâ”€â”€ kafka-clients-1.1.1.jar
â”‚   â””â”€â”€ spark-sql-kafka-0-10_2.11-2.3.2.jar
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kafka_producer.py                  # Le Producer Kafka (Python)
â”‚   â”œâ”€â”€ spark_streaming_job_kafka_hdfs.py  # Le Job Spark Streaming (Pyspark) KAFKA HDFS
â”‚   â””â”€â”€ spark_streaming_job_db.py          # Le Job Spark Streaming (Pyspark) KAFKA SUPABASE
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ reset_environment.sh      # Script de nettoyage (HDFS + Checkpoints)
â”‚
â””â”€â”€ README.md                     # Documentation du projet
```

---

## ğŸ“¦ Gestion des DÃ©pendances Python (PIP)

Sur la Sandbox HDP, il est nÃ©cessaire d'installer manuellement le gestionnaire de paquets `pip` pour pouvoir installer les librairies Python comme `kafka-python`.

### 1. Installation manuelle de PIP
Nous utilisons le script officiel d'installation (bootstrap) :

```bash
# 1. TÃ©lÃ©charger le script d'installation
curl "https://bootstrap.pypa.io/pip/2.7/get-pip.py" -o "get-pip.py"

# 2. ExÃ©cuter le script avec Python (root ou sudo peut Ãªtre requis selon les droits)
python get-pip.py

# 3. VÃ©rifier l'installation
pip --version
```
*(Note : Si vous utilisez Python 3, remplacez `python` par `python3`)*

### 2. Installation des librairies du projet
Une fois `pip` installÃ©, installez le connecteur Kafka nÃ©cessaire au script `kafka_producer.py`.

```bash
pip install kafka-python==2.0.2
```

---

## ğŸ› ï¸ PrÃ©-requis

* **Environnement** : Hortonworks Data Platform (HDP Sandbox) ou Cluster Spark/Kafka.
* **Spark** : Version 2.3+ (Compatible Structured Streaming).
* **Kafka** : Topic configurÃ©.
* **Python** : 2.7 (dans notre sandbox).
* **Postgres** : Supabase directement.

---

## ğŸš€ Installation et DÃ©marrage

### 1. Configuration de Kafka
CrÃ©ation du topic qui recevra les donnÃ©es brutes :

```bash
/usr/hdp/current/kafka-broker/bin/kafka-topics.sh \\
  --create \\
  --zookeeper sandbox-hdp.hortonworks.com:2181 \\
  --replication-factor 1 \\
  --partitions 1 \\
  --topic customers-raw
```

Et des deux autres topic (customers-raw ainsi que customers-alerts) de la mÃªme maniÃ¨re.

### 2. ExÃ©cution du Job Spark
#### 1. Dans le cas de KAFKA ou HDFS
Soumettez le job Ã  YARN ou en local via `spark-submit`. Notez l'utilisation des `.jars` pour le connecteur Kafka.

```bash
spark-submit \\
  --jars jars/spark-sql-kafka-0-10_2.11-2.3.2.jar,jars/kafka-clients-1.1.1.jar \\
  spark_streaming_job_kafka_hdfs.py
```

#### 2. Dans le cas d'une insertion en base de donnÃ©es
Soumettez le job Ã  YARN ou en local via `spark-submit`. Notez l'utilisation des `packages` pour spark.

```bash
spark-submit \\
  --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 \\
  spark_streaming_job_db.py
```

### 3. DÃ©marrage du Producer
Ce script va lire le fichier CSV depuis S3 avec boto3 et envoyer les messages un par un dans Kafka (le topic customers-raw).

```bash
python src/kafka_producer.py
```

---

## âš™ï¸ Configuration Technique & Robustesse

Ce projet a Ã©tÃ© configurÃ© pour gÃ©rer les erreurs courantes :

### 1. Gestion des Pertes de DonnÃ©es (Data Loss)
Kafka peut supprimer des anciens messages (rÃ©tention) avant que Spark ne les lise. Pour Ã©viter que le job ne crash avec une erreur `OffsetOutOfRangeException`, nous utilisons :

```python
.option("failOnDataLoss", "false")
```

### 2. Checkpointing (TolÃ©rance aux pannes)
Spark utilise des dossiers de checkpoints locaux pour sauvegarder l'Ã©tat du flux (offsets).
* **Chemin** : `/tmp/checkpoint_customers_...`
Cela garantit la sÃ©mantique **"Exactly-Once"** (aucun doublon, aucune perte) en cas de redÃ©marrage.

---

## ğŸ§¹ ProcÃ©dure de Reset (DÃ©pannage)

Si vous rencontrez des erreurs de type `Metadata Log` ou `IllegalStateException` (conflit entre le checkpoint et HDFS), ou si vous souhaitez relancer le traitement depuis le dÃ©but (offset 0), **suivez impÃ©rativement cette procÃ©dure de nettoyage** :

Pour ne pas se compliquer la tÃ¢che juste lancer le script : `reset_environnements.sh`
```bash
sh scripts/reset_environnements.sh
```

- Voici le detail des actions pour son exÃ©cution correcte

**1. ArrÃªter le Producer et le Job Spark (Ctrl+C).**

**2. Supprimer les mÃ©tadonnÃ©es locales (Le Cerveau) :**
```bash
rm -rf /tmp/checkpoint_customers_usa
rm -rf /tmp/checkpoint_customers_alerts
```

**3. Supprimer les donnÃ©es sur HDFS (La Destination) :**
```bash
hdfs dfs -rm -r /user/maria_dev/customers_usa
hdfs dfs -rm -r /user/maria_dev/customers_alerts
```

**4. Relancer le Producer puis le Job Spark.**

---

## ğŸ“Š VÃ©rification des RÃ©sultats

Pour vÃ©rifier que les donnÃ©es arrivent bien sur HDFS :

```bash
# Lister les fichiers
hdfs dfs -ls /user/maria_dev/customers_usa

# Lire le contenu d'un fichier (exemple)
hdfs dfs -cat /user/maria_dev/customers_usa/part-00000-....json
```
